{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**1. Import Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom typing import Tuple, Dict\nfrom scipy import linalg\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler # Added for preprocessing consistency","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:22:58.328475Z","iopub.execute_input":"2025-10-23T21:22:58.328858Z","iopub.status.idle":"2025-10-23T21:22:59.604173Z","shell.execute_reply.started":"2025-10-23T21:22:58.328831Z","shell.execute_reply":"2025-10-23T21:22:59.602466Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**2. Target-Aware ARS(Adaptive Residual Sampling) QRCP**","metadata":{}},{"cell_type":"code","source":"# Scikit-Learn Style Class of TAAQ\nclass TAAQ:\n    \"\"\"\n    Target-Aware ARS-QRCP (TAAQ) Feature Selection\n\n    3-Phase Algorithm:\n    - Phase 1: QRCP warm-up for k1\n    - Phase 2: Target-Projected Residual Sampling (TPRS) with one-step deflation\n    - Phase 3: Geometry-Predictive Pruning to size k\n    \"\"\"\n\n    def __init__(self, k: int, k1_fraction: float = 0.5, delta: int = 10,\n                epsilon: float = 1e-6, supervised: bool = True):\n        self.k = k\n        self.k1_fraction = k1_fraction\n        self.delta = delta\n        self.epsilon = epsilon\n        self.supervised = supervised\n        self.selected_features_ = None\n        self.runtime_ = {}\n\n    def fit(self, M: np.ndarray, y:np.ndarray) -> 'TAAQ':\n        \"\"\"\n        Fit TAAQ Selector (Model Trainer)\n        \"\"\"\n\n        # Stamp the start time\n        start_time = time.time()\n\n        # Preprocessing\n        M_std, y_std = self._preprocess(M, y)\n        m, n = M_std.shape\n\n        # Phase1: QRCP warm-up\n        t1 = time.time()\n        k1 = int(self.k * self.k1_fraction)\n        Q1, R11, S1 = self._phase1_partial_qrcp(M_std, k1)\n        self.runtime_['phase1'] = time.time() - t1\n\n        # Phase2: TPRS (Target-Projected Residual Sampling)\n        t2 = time.time()\n        S2_prime = self._phase2_tprs(M_std, y_std, Q1, S1, k1, self.delta)\n        self.runtime_['phase2'] = time.time() - t2\n\n        # Phase3: GPP (Geometry-Predictive Pruning)\n        t3 = time.time()\n        S_tmp = np.concatenate([S1, S2_prime])\n        S_final = self._phase3_gpp(M_std, y_std, S_tmp, self.k)\n        self.runtime_['phase3'] = time.time() - t3\n\n        self.selected_features_ = S_final\n        self.runtime_['total'] = time.time() - start_time\n\n        return self\n\n    def _preprocess(self, M: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Standardize columns of M and scale y.\n        \"\"\"\n        m, n = M.shape\n\n        # Standardize columns (axis=0): zero mean, unit variance\n        M_std = (M - M.mean(axis=0, keepdims=True)) / (M.std(axis=0, keepdims=True) + 1e-10)\n\n        # Center and scale y so that ||y||_2^2 = m\n        y_mean = y.mean()\n        y_sd = y.std()\n        y_std = (y - y_mean) / (y_sd + 1e-10)\n\n        return M_std, y_std\n\n    def _phase1_partial_qrcp(self, M: np.ndarray, k1: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Phase 1: Highly optimized Partial QRCP (k1 steps only).\n\n        Complexity: O(m*n*k1)\n        \"\"\"\n         \n        m, n = M.shape\n        \n        # Working copy\n        A = M.copy()\n        \n        # perm tracks the original column indices.\n        # perm[k] will store the original index of the k-th pivot column.\n        perm = np.arange(n)\n        \n        # Pre-compute the squared L2-norm of each column for pivoting.\n        # We use squared norms to avoid sqrt.\n        col_norms_full = np.sum(A**2, axis=0)\n        col_norms_partial = col_norms_full.copy()\n        \n        # Storage for the 'tau' scalars needed to reconstruct Q.\n        # H_k = I - tau_scalars[k] * v_k * v_k^T\n        tau_scalars = np.zeros(k1)\n\n        # main loop\n        for k in range(k1):\n            \n            # ====================\n            # 1. COLUMN PIVOTING\n            # ====================\n            # Find the column with the maximum remaining norm in the sub-array A[k:, k:].\n            j_max = k + np.argmax(col_norms_partial[k:])\n            \n            if j_max != k:\n                # Swap the pivot column (j_max) with the current column (k)\n                A[:, [k, j_max]] = A[:, [j_max, k]]\n                perm[[k, j_max]] = perm[[j_max, k]]\n                col_norms_full[[k, j_max]] = col_norms_full[[j_max, k]]\n                col_norms_partial[[k, j_max]] = col_norms_partial[[j_max, k]]\n            \n            # ========================================\n            # 2. HOUSEHOLDER QR STEP (on column k)\n            # ========================================\n            \n            # Get the k-th column vector starting from the diagonal.\n            x_k = A[k:m, k]\n            norm_x_k = np.linalg.norm(x_k)\n\n            if norm_x_k < 1e-14:\n                # This column is already (numerically) zero.\n                # No reflection needed.\n                tau_scalars[k] = 0\n                continue\n            \n            # 'sigma_k' is the target value for the first element,\n            # sign((x_k)_1) * ||x_k||.\n            sigma_k = -np.sign(x_k[0]) * norm_x_k if x_k[0] != 0 else -norm_x_k\n            \n            # --- Calculate the LAPACK-style Householder vector 'v' and 'tau' ---\n            \n            # 1. u_k = x_k - sigma_k * e_1\n            u_0 = x_k[0] - sigma_k\n            u_tail = x_k[1:]\n            \n            # 2. tau_k = -u_0 / sigma_k\n            tau_k = -u_0 / sigma_k\n            tau_scalars[k] = tau_k # Store for Q reconstruction\n            \n            # 3. v_k = u_k / u_0 = [1, u_tail / u_0]^T\n            v_tail = u_tail / u_0 if u_0 != 0 else u_tail # Avoid division by zero\n\n            # Apply the reflection H_k to all remaining columns (k+1 to n)\n            # A_sub = (I - tau_k * v_k * v_k^T) @ A_sub\n            if k < n - 1:\n                A_sub = A[k:m, k+1:n]\n                A_sub_head = A_sub[0, :]     # First row of the submatrix\n                A_sub_tail = A_sub[1:, :]    # Rest of the submatrix\n                \n                # w = (v_k^T @ A_sub)\n                #   = (v_k[0] * A_sub_head) + (v_tail^T @ A_sub_tail)\n                #   = (1 * A_sub_head) + (v_tail^T @ A_sub_tail)\n                w = A_sub_head + (v_tail @ A_sub_tail)\n                \n                # A_sub = A_sub - tau_k * v_k * w^T\n                A[k, k+1:n] -= tau_k * w              # Update head (v_k[0]=1)\n                A[k+1:m, k+1:n] -= tau_k * np.outer(v_tail, w) # Update tail\n            \n            # Store R_kk and the Householder vector v_tail\n            # directly into the matrix A.\n            A[k, k] = sigma_k      # R's diagonal element\n            A[k+1:m, k] = v_tail # The tail of the vector v_k\n            \n            # ===================================\n            # 3. UPDATE COLUMN NORMS (downdate)\n            # ===================================\n            # This is the O(n) part.\n            # Use Pythagoras to update the remaining norm of each column after the reflection.\n            # new_tail_norm^2 = total_norm^2 - new_head_val^2\n            if k < n - 1:\n                for j in range(k+1, n):\n                    # temp = 1 - (A[k, j] / col_norms_partial[j])**2\n                    temp_ratio = A[k, j] / col_norms_partial[j] if col_norms_partial[j] > 0 else 0\n                    temp_ratio = max(0, (1 - temp_ratio) * (1 + temp_ratio)) # (1-t^2)\n                    \n                    # col_norms_partial[j] = col_norms_partial[j] * sqrt(temp_ratio)\n                    col_norms_partial[j] *= np.sqrt(temp_ratio)\n                    \n                    # Failsafe: Recompute norm if numerical error accumulates\n                    if col_norms_partial[j] < 0.1 * np.sqrt(col_norms_full[j]):\n                        col_norms_partial[j] = np.linalg.norm(A[k+1:m, j])\n                        col_norms_full[j] = col_norms_partial[j]**2\n        \n        # ====================================================================\n        # 4. EXTRACT RESULTS (Q, R, S)\n        # ====================================================================\n        \n        # R11 is the upper-triangular k1 x k1 block of A\n        R11 = np.triu(A[:k1, :k1])\n\n        Q1 = np.eye(m, k1)\n        \n        for k in range(k1-1, -1, -1):\n            tau_k = tau_scalars[k] # Get the stored scalar\n            \n            if tau_k != 0:\n                # Reconstruct v_k = [1, v_tail]^T\n                v = np.zeros(m - k)\n                v[0] = 1\n                v[1:] = A[k+1:m, k] # Get the stored v_tail\n                \n                # Apply the transformation:\n                # Q1[k:, k:] = H_k @ Q1[k:, k:]\n                #            = (I - tau_k * v * v^T) @ Q1[k:, k:]\n                w = tau_k * (v @ Q1[k:, k:])\n                Q1[k:, k:] -= np.outer(v, w)\n        \n        # S1: The original indices of the first k1 pivot columns\n        S1 = perm[:k1]\n        \n        return Q1, R11, S1\n    \n    def _phase2_tprs(self, M: np.ndarray, y: np.ndarray, Q1: np.ndarray,\n                    S1: np.ndarray, k1: int, delta: int) -> np.ndarray:\n        \"\"\"\n        Phase 2: Target-Projected Residual Sampling with one-step \n        \"\"\"\n        m, n = M.shape\n        k2 = self.k - k1\n        # ==========\n        # 1. TPRS\n        # ==========\n        # Target residual\n        r_y = y - Q1 @ (Q1.T @ y)\n\n        # Precompute residualized features\n        Q1_T_M = Q1.T @ M\n\n        # b_j = \\|x_j\\|_2^2 - \\|Q_1^Tx_j\\|_2^2\n        b = np.sum(M**2, axis=0) - np.sum(Q1_T_M**2, axis=0)\n        b = np.maximum(b, self.epsilon) # Prevent 0\n\n        # s_j = r_y^TP_\\perpx_j = r_y^Tx_j\n        s = r_y @ M\n\n        # Supervised score a_j := s_j^2 / (b_j + \\epsilon)\n        if self.supervised:\n            a = s**2 / (b + self.epsilon)\n        else:\n            a = b\n        # ==================================================\n        # 2. Greedy Oversampling with one-step deflation\n        # ==================================================\n                \n        # Initialize candidate pool\n        valid_mask = np.ones(n, dtype=bool)\n        valid_mask[S1] = False\n\n        # List for chosen columns in phase 2\n        S2_prime = []\n\n        for _ in range(k2+delta):\n            a_masked = np.where(valid_mask, a, -np.inf)\n\n            # j* = argmax a_j\n            j_star = np.argmax(a_masked)\n\n            # Add it to the answer\n            S2_prime.append(j_star)\n\n            # Update for the next iteration\n            valid_mask[j_star] = False\n\n            # ----- One-Step Deflation ----- #\n\n            # Choose the column for the chosen j\n            x_j = M[:, j_star]\n\n            # Get P_\\perp x_j\n            P_perp_x = x_j - Q1 @ (Q1.T @ x_j)\n\n            # Normalize it\n            norm_P_perp_x = np.linalg.norm(P_perp_x)\n\n            if norm_P_perp_x > 1e-10:\n\n                # New direction (contribution) of x_j\n                u = P_perp_x / norm_P_perp_x\n\n                # The Magnitude * direction = The new contribution of x_j\n                g = u.T @ M\n\n                # The new columns contribution to the label\n                alpha = u.T @ r_y\n\n                # Decrease the residual\n                r_y -= alpha * u\n\n                # Decrease the correlation of each columns\n                s -= alpha * g\n\n                if self.supervised:\n                    a = s**2 / (b+self.epsilon)\n\n                else:\n                    a = b     \n\n        return np.array(S2_prime, dtype=int)\n\n    def _phase3_gpp(self, M: np.ndarray, y: np.ndarray,\n                   S_tmp: np.ndarray, k:int) -> np.ndarray:\n        \"\"\"\n        Phase 3: Geometry-Predictive Pruning to size k.\n        \"\"\"\n        # Working copy\n        S_current = S_tmp.copy()\n\n        # Measure delta that we are going to prune\n        delta = len(S_tmp) - k\n\n        for _ in range(delta):\n            C = M[:, S_current]\n\n            # QR Decomposition\n            # mode='economic' --> thin QR (Q = m*n, R = n*n)\n            Q, R = linalg.qr(C, mode='economic')\n\n            # Compute beta (R^-1(Q^T y)) --> \"Predictive\"\n            # We don't get inverse, we use \"back substitution\" (solve_triangular)\n            # \\beta = min\\|C\\beta-y\\|_2^2\n            # Least Square Problem\n            # Equation: C\\beta = y\n            # Solution: \\beta = (C^TC)^-1C^Ty\n            # \\beta = (R^TQ^TQ^R)^-1R^TQ^Ty = (R^TR)^-1R^TQ^Ty = R^-1Q^Ty\n            beta = linalg.solve_triangular(R, Q.T @ y, lower=False)\n\n            # Compute r_i --> \"Geometric Redundancy\" (Numerical Instability)\n            # VIF (Variance Inflaction Factor) = 1/ (1 - R_i)\n            # C = QR --> G = C^TC = R^TR\n            # r_i = \\|R^-Te_i\\|_2^2 = (R^-Te_i)^T(R^-Te_i) = e_i^T(R^-1R^-T)e_i = e_i^T(R^TR)^-1e_i = [(C^TC)^-1]_ii\n            # By block matrix inversion --> (G^-1)_ii = 1 / c_i^Tc_i(1-R_i^2) = 1/\\|c_i\\|_2^2 \\times VIF_i\n            R_inv_T = linalg.solve_triangular(R, np.eye(len(S_current)), trans='T', lower=False)\n            r_i = np.sum(R_inv_T**2, axis=0)\n\n            # Compute drop scores: d_i = r_i / (beta_i^2 + epsilon)\n            d_i = r_i / (beta**2 + self.epsilon)\n\n            # Remove worst feature\n            i_star = np.argmax(d_i)\n            S_current = np.delete(S_current, i_star) # Delete the column\n\n        return S_current\n        \n    def transform(self, M: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Select features from M.\n        \"\"\"\n        \n        if self.selected_features_ is None:\n            raise ValueError(\"Must fit before transform\")\n            \n        return M[:, self.selected_features_]  \n        \n    def fit_transform(self, M: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Fit and transform in one step.\n        \"\"\"\n        return self.fit(M, y).transform(M)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:22:59.606119Z","iopub.execute_input":"2025-10-23T21:22:59.606655Z","iopub.status.idle":"2025-10-23T21:22:59.739863Z","shell.execute_reply.started":"2025-10-23T21:22:59.606619Z","shell.execute_reply":"2025-10-23T21:22:59.738679Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**3. Baseline Methods**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Evaluation Framework**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Data Generators**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6. Experiments**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**7. Quick Test**","metadata":{}},{"cell_type":"code","source":"def run_simple_experiment():\n    \"\"\"Runs a basic test of the TAAQ class.\"\"\"\n\n    print(\"--- Running Simple TAAQ Experiment ---\")\n\n    # 1. Generate synthetic data\n    print(\"Generating synthetic data...\")\n    m, n = 500, 100 # Samples, Features\n    k_true = 15     # Number of informative features in the data\n    X, y = make_regression(n_samples=m, n_features=n, n_informative=k_true,\n                            noise=10, random_state=42)\n    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n\n    # 2. Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n    print(f\"Test shapes:  X={X_test.shape}, y={y_test.shape}\")\n\n    # 3. Instantiate and Fit TAAQ\n    k_select = 20 # Target number of features to select\n    print(f\"\\nInitializing TAAQ to select k={k_select} features...\")\n    taaq_selector = TAAQ(k=k_select, k1_fraction=0.5, delta=5, supervised=True)\n\n    print(\"Fitting TAAQ selector on training data...\")\n    start_fit = time.time()\n    taaq_selector.fit(X_train, y_train)\n    fit_time = time.time() - start_fit\n    print(f\"Fit completed in {fit_time:.4f} seconds.\")\n\n    # Print selected features and runtime breakdown\n    print(\"Selected feature indices:\", taaq_selector.selected_features_)\n    print(\"Runtime breakdown (seconds):\")\n    for phase, t in taaq_selector.runtime_.items():\n        print(f\"  - {phase}: {t:.4f}\")\n\n    # 4. Transform the data\n    print(\"\\nTransforming train and test data using selected features...\")\n    X_train_taaq = taaq_selector.transform(X_train)\n    X_test_taaq = taaq_selector.transform(X_test)\n    print(f\"Transformed train shape: {X_train_taaq.shape}\")\n    print(f\"Transformed test shape:  {X_test_taaq.shape}\")\n\n    # 5. Train and evaluate a simple Ridge model\n    print(\"\\nTraining Ridge model on TAAQ-selected features...\")\n    model = Ridge(alpha=1.0)\n    model.fit(X_train_taaq, y_train)\n\n    print(\"Evaluating model on transformed test data...\")\n    y_pred = model.predict(X_test_taaq)\n    r2 = r2_score(y_test, y_pred)\n    print(f\"\\nR² score on test set using TAAQ features: {r2:.4f}\")\n\n    # --- Optional: Compare with using all features ---\n    print(\"\\nTraining Ridge model on ALL features for comparison...\")\n    model_all = Ridge(alpha=1.0)\n    # Need to scale data if comparing fairly, especially for Ridge\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    model_all.fit(X_train_scaled, y_train)\n    y_pred_all = model_all.predict(X_test_scaled)\n    r2_all = r2_score(y_test, y_pred_all)\n    print(f\"R² score on test set using ALL features: {r2_all:.4f}\")\n\n    print(\"\\n--- Experiment Finished ---\")\n\n\n# ============================================================================\n# Run the experiment\n# ============================================================================\nif __name__ == \"__main__\":\n    run_simple_experiment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T21:22:59.740932Z","iopub.execute_input":"2025-10-23T21:22:59.741238Z","iopub.status.idle":"2025-10-23T21:22:59.946876Z","shell.execute_reply.started":"2025-10-23T21:22:59.741213Z","shell.execute_reply":"2025-10-23T21:22:59.945480Z"}},"outputs":[{"name":"stdout","text":"--- Running Simple TAAQ Experiment ---\nGenerating synthetic data...\nData shape: X=(500, 100), y=(500,)\nTrain shapes: X=(350, 100), y=(350,)\nTest shapes:  X=(150, 100), y=(150,)\n\nInitializing TAAQ to select k=20 features...\nFitting TAAQ selector on training data...\nFit completed in 0.0834 seconds.\nSelected feature indices: [56 62 74 45 80 81  2 83 70 54 89 49 11 59 66 29 57 82 85 16]\nRuntime breakdown (seconds):\n  - phase1: 0.0243\n  - phase2: 0.0271\n  - phase3: 0.0210\n  - total: 0.0834\n\nTransforming train and test data using selected features...\nTransformed train shape: (350, 20)\nTransformed test shape:  (150, 20)\n\nTraining Ridge model on TAAQ-selected features...\nEvaluating model on transformed test data...\n\nR² score on test set using TAAQ features: 0.9981\n\nTraining Ridge model on ALL features for comparison...\nR² score on test set using ALL features: 0.9971\n\n--- Experiment Finished ---\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**8. Main Execution**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}