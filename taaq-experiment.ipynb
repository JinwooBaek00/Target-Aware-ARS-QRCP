{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-23T21:22:58.328858Z",
     "iopub.status.busy": "2025-10-23T21:22:58.328475Z",
     "iopub.status.idle": "2025-10-23T21:22:59.604173Z",
     "shell.execute_reply": "2025-10-23T21:22:59.602466Z",
     "shell.execute_reply.started": "2025-10-23T21:22:58.328831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Tuple, Dict\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LassoCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler # Added for preprocessing consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Target-Aware ARS(Adaptive Residual Sampling) QRCP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T21:22:59.606655Z",
     "iopub.status.busy": "2025-10-23T21:22:59.606119Z",
     "iopub.status.idle": "2025-10-23T21:22:59.739863Z",
     "shell.execute_reply": "2025-10-23T21:22:59.738679Z",
     "shell.execute_reply.started": "2025-10-23T21:22:59.606619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn Style Class of TAAQ\n",
    "class TAAQ:\n",
    "    \"\"\"\n",
    "    Target-Aware ARS-QRCP (TAAQ) Feature Selection\n",
    "\n",
    "    3-Phase Algorithm:\n",
    "    - Phase 1: QRCP warm-up for k1\n",
    "    - Phase 2: Target-Projected Residual Sampling (TPRS) with one-step deflation\n",
    "    - Phase 3: Geometry-Predictive Pruning to size k\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int, k1_fraction: float = 0.5, delta: int = 10,\n",
    "                epsilon: float = 1e-6, supervised: bool = True):\n",
    "        self.k = k\n",
    "        self.k1_fraction = k1_fraction\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        self.supervised = supervised\n",
    "        self.selected_features_ = None\n",
    "        self.runtime_ = {}\n",
    "\n",
    "    def fit(self, M: np.ndarray, y:np.ndarray) -> 'TAAQ':\n",
    "        \"\"\"\n",
    "        Fit TAAQ Selector (Model Trainer)\n",
    "        \"\"\"\n",
    "\n",
    "        # Stamp the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Preprocessing\n",
    "        M_std, y_std = self._preprocess(M, y)\n",
    "        m, n = M_std.shape\n",
    "\n",
    "        # Phase1: QRCP warm-up\n",
    "        t1 = time.time()\n",
    "        k1 = int(self.k * self.k1_fraction)\n",
    "        Q1, R11, S1 = self._phase1_partial_qrcp(M_std, k1)\n",
    "        self.runtime_['phase1'] = time.time() - t1\n",
    "\n",
    "        # Phase2: TPRS (Target-Projected Residual Sampling)\n",
    "        t2 = time.time()\n",
    "        S2_prime = self._phase2_tprs(M_std, y_std, Q1, S1, k1, self.delta)\n",
    "        self.runtime_['phase2'] = time.time() - t2\n",
    "\n",
    "        # Phase3: GPP (Geometry-Predictive Pruning)\n",
    "        t3 = time.time()\n",
    "        S_tmp = np.concatenate([S1, S2_prime])\n",
    "        S_final = self._phase3_gpp(M_std, y_std, S_tmp, self.k)\n",
    "        self.runtime_['phase3'] = time.time() - t3\n",
    "\n",
    "        self.selected_features_ = S_final\n",
    "        self.runtime_['total'] = time.time() - start_time\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _preprocess(self, M: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Standardize columns of M and scale y.\n",
    "        \"\"\"\n",
    "        m, n = M.shape\n",
    "\n",
    "        # Standardize columns (axis=0): zero mean, unit variance\n",
    "        M_std = (M - M.mean(axis=0, keepdims=True)) / (M.std(axis=0, keepdims=True) + 1e-10)\n",
    "\n",
    "        # Center and scale y so that ||y||_2^2 = m\n",
    "        y_mean = y.mean()\n",
    "        y_sd = y.std()\n",
    "        y_std = (y - y_mean) / (y_sd + 1e-10)\n",
    "\n",
    "        return M_std, y_std\n",
    "\n",
    "    def _phase1_partial_qrcp(self, M: np.ndarray, k1: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Phase 1: Highly optimized Partial QRCP (k1 steps only).\n",
    "\n",
    "        Complexity: O(m*n*k1)\n",
    "        \"\"\"\n",
    "         \n",
    "        m, n = M.shape\n",
    "        \n",
    "        # Working copy\n",
    "        A = M.copy()\n",
    "        \n",
    "        # perm tracks the original column indices.\n",
    "        # perm[k] will store the original index of the k-th pivot column.\n",
    "        perm = np.arange(n)\n",
    "        \n",
    "        # Pre-compute the squared L2-norm of each column for pivoting.\n",
    "        # We use squared norms to avoid sqrt.\n",
    "        col_norms_full = np.sum(A**2, axis=0)\n",
    "        col_norms_partial = col_norms_full.copy()\n",
    "        \n",
    "        # Storage for the 'tau' scalars needed to reconstruct Q.\n",
    "        # H_k = I - tau_scalars[k] * v_k * v_k^T\n",
    "        tau_scalars = np.zeros(k1)\n",
    "\n",
    "        # main loop\n",
    "        for k in range(k1):\n",
    "            \n",
    "            # ====================\n",
    "            # 1. COLUMN PIVOTING\n",
    "            # ====================\n",
    "            # Find the column with the maximum remaining norm in the sub-array A[k:, k:].\n",
    "            j_max = k + np.argmax(col_norms_partial[k:])\n",
    "            \n",
    "            if j_max != k:\n",
    "                # Swap the pivot column (j_max) with the current column (k)\n",
    "                A[:, [k, j_max]] = A[:, [j_max, k]]\n",
    "                perm[[k, j_max]] = perm[[j_max, k]]\n",
    "                col_norms_full[[k, j_max]] = col_norms_full[[j_max, k]]\n",
    "                col_norms_partial[[k, j_max]] = col_norms_partial[[j_max, k]]\n",
    "            \n",
    "            # ========================================\n",
    "            # 2. HOUSEHOLDER QR STEP (on column k)\n",
    "            # ========================================\n",
    "            \n",
    "            # Get the k-th column vector starting from the diagonal.\n",
    "            x_k = A[k:m, k]\n",
    "            norm_x_k = np.linalg.norm(x_k)\n",
    "\n",
    "            if norm_x_k < 1e-14:\n",
    "                # This column is already (numerically) zero.\n",
    "                # No reflection needed.\n",
    "                tau_scalars[k] = 0\n",
    "                continue\n",
    "            \n",
    "            # 'sigma_k' is the target value for the first element,\n",
    "            # sign((x_k)_1) * ||x_k||.\n",
    "            sigma_k = -np.sign(x_k[0]) * norm_x_k if x_k[0] != 0 else -norm_x_k\n",
    "            \n",
    "            # --- Calculate the LAPACK-style Householder vector 'v' and 'tau' ---\n",
    "            \n",
    "            # 1. u_k = x_k - sigma_k * e_1\n",
    "            u_0 = x_k[0] - sigma_k\n",
    "            u_tail = x_k[1:]\n",
    "            \n",
    "            # 2. tau_k = -u_0 / sigma_k\n",
    "            tau_k = -u_0 / sigma_k\n",
    "            tau_scalars[k] = tau_k # Store for Q reconstruction\n",
    "            \n",
    "            # 3. v_k = u_k / u_0 = [1, u_tail / u_0]^T\n",
    "            v_tail = u_tail / u_0 if u_0 != 0 else u_tail # Avoid division by zero\n",
    "\n",
    "            # Apply the reflection H_k to all remaining columns (k+1 to n)\n",
    "            # A_sub = (I - tau_k * v_k * v_k^T) @ A_sub\n",
    "            if k < n - 1:\n",
    "                A_sub = A[k:m, k+1:n]\n",
    "                A_sub_head = A_sub[0, :]     # First row of the submatrix\n",
    "                A_sub_tail = A_sub[1:, :]    # Rest of the submatrix\n",
    "                \n",
    "                # w = (v_k^T @ A_sub)\n",
    "                #   = (v_k[0] * A_sub_head) + (v_tail^T @ A_sub_tail)\n",
    "                #   = (1 * A_sub_head) + (v_tail^T @ A_sub_tail)\n",
    "                w = A_sub_head + (v_tail @ A_sub_tail)\n",
    "                \n",
    "                # A_sub = A_sub - tau_k * v_k * w^T\n",
    "                A[k, k+1:n] -= tau_k * w              # Update head (v_k[0]=1)\n",
    "                A[k+1:m, k+1:n] -= tau_k * np.outer(v_tail, w) # Update tail\n",
    "            \n",
    "            # Store R_kk and the Householder vector v_tail\n",
    "            # directly into the matrix A.\n",
    "            A[k, k] = sigma_k      # R's diagonal element\n",
    "            A[k+1:m, k] = v_tail # The tail of the vector v_k\n",
    "            \n",
    "            # ===================================\n",
    "            # 3. UPDATE COLUMN NORMS (downdate)\n",
    "            # ===================================\n",
    "            # This is the O(n) part.\n",
    "            # Use Pythagoras to update the remaining norm of each column after the reflection.\n",
    "            # new_tail_norm^2 = total_norm^2 - new_head_val^2\n",
    "            if k < n - 1:\n",
    "                for j in range(k+1, n):\n",
    "                    # temp = 1 - (A[k, j] / col_norms_partial[j])**2\n",
    "                    temp_ratio = A[k, j] / col_norms_partial[j] if col_norms_partial[j] > 0 else 0\n",
    "                    temp_ratio = max(0, (1 - temp_ratio) * (1 + temp_ratio)) # (1-t^2)\n",
    "                    \n",
    "                    # col_norms_partial[j] = col_norms_partial[j] * sqrt(temp_ratio)\n",
    "                    col_norms_partial[j] *= np.sqrt(temp_ratio)\n",
    "                    \n",
    "                    # Failsafe: Recompute norm if numerical error accumulates\n",
    "                    if col_norms_partial[j] < 0.1 * np.sqrt(col_norms_full[j]):\n",
    "                        col_norms_partial[j] = np.linalg.norm(A[k+1:m, j])\n",
    "                        col_norms_full[j] = col_norms_partial[j]**2\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 4. EXTRACT RESULTS (Q, R, S)\n",
    "        # ====================================================================\n",
    "        \n",
    "        # R11 is the upper-triangular k1 x k1 block of A\n",
    "        R11 = np.triu(A[:k1, :k1])\n",
    "\n",
    "        Q1 = np.eye(m, k1)\n",
    "        \n",
    "        for k in range(k1-1, -1, -1):\n",
    "            tau_k = tau_scalars[k] # Get the stored scalar\n",
    "            \n",
    "            if tau_k != 0:\n",
    "                # Reconstruct v_k = [1, v_tail]^T\n",
    "                v = np.zeros(m - k)\n",
    "                v[0] = 1\n",
    "                v[1:] = A[k+1:m, k] # Get the stored v_tail\n",
    "                \n",
    "                # Apply the transformation:\n",
    "                # Q1[k:, k:] = H_k @ Q1[k:, k:]\n",
    "                #            = (I - tau_k * v * v^T) @ Q1[k:, k:]\n",
    "                w = tau_k * (v @ Q1[k:, k:])\n",
    "                Q1[k:, k:] -= np.outer(v, w)\n",
    "        \n",
    "        # S1: The original indices of the first k1 pivot columns\n",
    "        S1 = perm[:k1]\n",
    "        \n",
    "        return Q1, R11, S1\n",
    "    \n",
    "    def _phase2_tprs(self, M: np.ndarray, y: np.ndarray, Q1: np.ndarray,\n",
    "                    S1: np.ndarray, k1: int, delta: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Phase 2: Target-Projected Residual Sampling with one-step \n",
    "        \"\"\"\n",
    "        m, n = M.shape\n",
    "        k2 = self.k - k1\n",
    "        # ==========\n",
    "        # 1. TPRS\n",
    "        # ==========\n",
    "        # Target residual\n",
    "        r_y = y - Q1 @ (Q1.T @ y)\n",
    "\n",
    "        # Precompute residualized features\n",
    "        Q1_T_M = Q1.T @ M\n",
    "\n",
    "        # b_j = \\|x_j\\|_2^2 - \\|Q_1^Tx_j\\|_2^2\n",
    "        b = np.sum(M**2, axis=0) - np.sum(Q1_T_M**2, axis=0)\n",
    "        b = np.maximum(b, self.epsilon) # Prevent 0\n",
    "\n",
    "        # s_j = r_y^TP_\\perpx_j = r_y^Tx_j\n",
    "        s = r_y @ M\n",
    "\n",
    "        # Supervised score a_j := s_j^2 / (b_j + \\epsilon)\n",
    "        if self.supervised:\n",
    "            a = s**2 / (b + self.epsilon)\n",
    "        else:\n",
    "            a = b\n",
    "        # ==================================================\n",
    "        # 2. Greedy Oversampling with one-step deflation\n",
    "        # ==================================================\n",
    "                \n",
    "        # Initialize candidate pool\n",
    "        valid_mask = np.ones(n, dtype=bool)\n",
    "        valid_mask[S1] = False\n",
    "\n",
    "        # List for chosen columns in phase 2\n",
    "        S2_prime = []\n",
    "\n",
    "        for _ in range(k2+delta):\n",
    "            a_masked = np.where(valid_mask, a, -np.inf)\n",
    "\n",
    "            # j* = argmax a_j\n",
    "            j_star = np.argmax(a_masked)\n",
    "\n",
    "            # Add it to the answer\n",
    "            S2_prime.append(j_star)\n",
    "\n",
    "            # Update for the next iteration\n",
    "            valid_mask[j_star] = False\n",
    "\n",
    "            # ----- One-Step Deflation ----- #\n",
    "\n",
    "            # Choose the column for the chosen j\n",
    "            x_j = M[:, j_star]\n",
    "\n",
    "            # Get P_\\perp x_j\n",
    "            P_perp_x = x_j - Q1 @ (Q1.T @ x_j)\n",
    "\n",
    "            # Normalize it\n",
    "            norm_P_perp_x = np.linalg.norm(P_perp_x)\n",
    "\n",
    "            if norm_P_perp_x > 1e-10:\n",
    "\n",
    "                # New direction (contribution) of x_j\n",
    "                u = P_perp_x / norm_P_perp_x\n",
    "\n",
    "                # The Magnitude * direction = The new contribution of x_j\n",
    "                g = u.T @ M\n",
    "\n",
    "                # The new columns contribution to the label\n",
    "                alpha = u.T @ r_y\n",
    "\n",
    "                # Decrease the residual\n",
    "                r_y -= alpha * u\n",
    "\n",
    "                # Decrease the correlation of each columns\n",
    "                s -= alpha * g\n",
    "\n",
    "                if self.supervised:\n",
    "                    a = s**2 / (b+self.epsilon)\n",
    "\n",
    "                else:\n",
    "                    a = b     \n",
    "\n",
    "        return np.array(S2_prime, dtype=int)\n",
    "\n",
    "    def _phase3_gpp(self, M: np.ndarray, y: np.ndarray,\n",
    "                   S_tmp: np.ndarray, k:int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Phase 3: Geometry-Predictive Pruning to size k.\n",
    "        \"\"\"\n",
    "        # Working copy\n",
    "        S_current = S_tmp.copy()\n",
    "\n",
    "        # Measure delta that we are going to prune\n",
    "        delta = len(S_tmp) - k\n",
    "\n",
    "        for _ in range(delta):\n",
    "            C = M[:, S_current]\n",
    "\n",
    "            # QR Decomposition\n",
    "            # mode='economic' --> thin QR (Q = m*n, R = n*n)\n",
    "            Q, R = linalg.qr(C, mode='economic')\n",
    "\n",
    "            # Compute beta (R^-1(Q^T y)) --> \"Predictive\"\n",
    "            # We don't get inverse, we use \"back substitution\" (solve_triangular)\n",
    "            # \\beta = min\\|C\\beta-y\\|_2^2\n",
    "            # Least Square Problem\n",
    "            # Equation: C\\beta = y\n",
    "            # Solution: \\beta = (C^TC)^-1C^Ty\n",
    "            # \\beta = (R^TQ^TQ^R)^-1R^TQ^Ty = (R^TR)^-1R^TQ^Ty = R^-1Q^Ty\n",
    "            beta = linalg.solve_triangular(R, Q.T @ y, lower=False)\n",
    "\n",
    "            # Compute r_i --> \"Geometric Redundancy\" (Numerical Instability)\n",
    "            # VIF (Variance Inflaction Factor) = 1/ (1 - R_i)\n",
    "            # C = QR --> G = C^TC = R^TR\n",
    "            # r_i = \\|R^-Te_i\\|_2^2 = (R^-Te_i)^T(R^-Te_i) = e_i^T(R^-1R^-T)e_i = e_i^T(R^TR)^-1e_i = [(C^TC)^-1]_ii\n",
    "            # By block matrix inversion --> (G^-1)_ii = 1 / c_i^Tc_i(1-R_i^2) = 1/\\|c_i\\|_2^2 \\times VIF_i\n",
    "            R_inv_T = linalg.solve_triangular(R, np.eye(len(S_current)), trans='T', lower=False)\n",
    "            r_i = np.sum(R_inv_T**2, axis=0)\n",
    "\n",
    "            # Compute drop scores: d_i = r_i / (beta_i^2 + epsilon)\n",
    "            d_i = r_i / (beta**2 + self.epsilon)\n",
    "\n",
    "            # Remove worst feature\n",
    "            i_star = np.argmax(d_i)\n",
    "            S_current = np.delete(S_current, i_star) # Delete the column\n",
    "\n",
    "        return S_current\n",
    "        \n",
    "    def transform(self, M: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Select features from M.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.selected_features_ is None:\n",
    "            raise ValueError(\"Must fit before transform\")\n",
    "            \n",
    "        return M[:, self.selected_features_]  \n",
    "        \n",
    "    def fit_transform(self, M: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit and transform in one step.\n",
    "        \"\"\"\n",
    "        return self.fit(M, y).transform(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Baseline Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RRQR:\n",
    "    \"\"\"\n",
    "    Strong Rank-Revealing QR Decomposition.\n",
    "\n",
    "    Produces A*Pi = Q*R where:\n",
    "    - R11 (leading black) is well-conditioned.\n",
    "    - R22 (training block) is small (low residual energy)\n",
    "    - Coupling ||R11^-1 R12|| is small\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f = 2.0, max_iter = 10):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        f : float\n",
    "            Tolerance for maximum entry in coupling matrix T.\n",
    "            If max(abs(T)) <= f, decompisition is considered rank-revealing.\n",
    "        max_iter : int\n",
    "            Maximum number of column swap iterations\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.max_iter = max_iter\n",
    "        self.Q = None\n",
    "        self.R = None\n",
    "        self.P = None\n",
    "\n",
    "    def fit(self, A, k = None):\n",
    "        \"\"\"\n",
    "        Compute strong RRQR factorization.\n",
    "\n",
    "        Parameters:\n",
    "        A : array_like, shape (m, n)\n",
    "            Input matrix\n",
    "        k : int, optional\n",
    "            Target rank. If None, use full rank\n",
    "        \"\"\"\n",
    "        A = np.array(A, dtype = float)\n",
    "        m, n = A.shape\n",
    "        if k is None:\n",
    "            k = min(m, n)\n",
    "\n",
    "        #### Step 1: initial QRCP Execution ####\n",
    "        # standard QR with column pivoting (weak RRQR)\n",
    "        # Q is an orthogonal square matrix\n",
    "        # R is an upper triangular \n",
    "        # P is a permutation matrix that reorders columns of A\n",
    "        Q, R, P = qr(A, pivoting=True, mode='full')\n",
    "        P = np.array(P)\n",
    "\n",
    "        iter_count = 0\n",
    "        while iter_count < self.max_iter:\n",
    "            #### Step 2: Partition R into leading and trailing blocks ####\n",
    "            \n",
    "            # R11: leading kxk block <- this are the first k selected columms\n",
    "            # R12: trailing k x (n-k) block <- decribes how much of the remaining columns depend on leading ones\n",
    "            R11 = R[:k, :k]\n",
    "            R12 = R[:k, :k]\n",
    "\n",
    "            #### Step 3: Compute the Coupling Matrix T ####\n",
    "            #### “strong” feature reordering ####\n",
    "            # Solve R11 * T = R12 for T (triangular solve)\n",
    "            # T tells us how much the trailing columns depend on the leading ones\n",
    "            if R12.shape[1] == 0:\n",
    "                break\n",
    "            T = solve_triangular(R11, R12)\n",
    "\n",
    "            #### Step 4: Check the largest element of T for tolerance ####\n",
    "            i_star, j_star = np.unravel_index(np.abs(T).argmax(), T.shape)\n",
    "            max_T = np.abs(T[i_star, j_star])\n",
    "\n",
    "            if max_T <= self.f:\n",
    "                # Coupling is small enough -> decomposition is rank-revealing\n",
    "                break\n",
    "            \n",
    "            #### Step 5: Correction Plan - Swap columns based on max entry in T ####\n",
    "            # Swap columns: i_star in R11 (leading block), j_star in R12 (trailing block)\n",
    "            # If some trailing column depends too much (i.e., large entries in T), we swap that column forward\n",
    "            col1 = i_star\n",
    "            col2 = k + j_star\n",
    "            R[:, [col1, col2]] = R[:, [col2, col1]]     # swap columns\n",
    "            P[[col1, col2]] = P[[col2, col1]]           # update permuation\n",
    "\n",
    "            #### Step 6: Restore upper triangular structure with a simple QR on affected columns ####\n",
    "            # After the column wrap, R is not upper triangular \n",
    "            # Re-factor the affected block with QR to zero out the subdiagonal\n",
    "            rows = slice(i_star, m)\n",
    "            cols = slice(col1, col2 + 2)\n",
    "            Q_local, R_local = qr(R[rows, cols], mode = 'economic')\n",
    "            R[rows, cols] = R_local     # updated affected submatrix\n",
    "            # Q update is ignored for simplicity\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "        #### Step 7: Recompute Q for the permutated matrix ####\n",
    "        # After all swaps, recompute Q to match final R and permutation\n",
    "        self.Q, R_full = qr(A[:, P], mode = 'full')\n",
    "        self.R = R_full\n",
    "        # P holds the final column permutation indices, ranking from most to least \n",
    "        self.P = P\n",
    "\n",
    "        return self\n",
    "         \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply RRQR transformation to a new matrix X\n",
    "        \"\"\"\n",
    "        if self.Q is None:\n",
    "            raise ValueError(\"Must fit first.\")\n",
    "        return self.Q.T @ X[:, self.P]\n",
    "\n",
    "class QRCP:\n",
    "    \"\"\"\n",
    "    QR Decomposition with Column Pivoting (QRCP) \n",
    "\n",
    "    Produces a weak rank-revealing decomposition:\n",
    "        X[:, P] = Q @ R\n",
    "    where:\n",
    "        - Q is orthogonal\n",
    "        - R is upper-triangular\n",
    "        - P is a permutation array (pivot indices)\n",
    "    \n",
    "    Can select the top-k columns based on pivoting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k: int = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        top_k : int, optional\n",
    "            Number of top features (columns) to select. If None, select all.\n",
    "        \"\"\"\n",
    "        self.top_k = top_k                # Target number of features\n",
    "        self.Q = None                     # Orthogonal matrix from QR\n",
    "        self.R = None                     # Upper-triangular matrix from QR\n",
    "        self.P = None                     # Column permutation indices\n",
    "        self.selected_features_ = None    # Indices of top-selected features\n",
    "        self.runtime_ = {}                # Dictionary to track runtimes\n",
    "\n",
    "    def fit(self, X: np.ndarray, y=None) -> 'QRCP':\n",
    "        \"\"\"\n",
    "        Fit the QRCP model to the data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (m, n)\n",
    "            Input feature matrix\n",
    "        y : ignored\n",
    "            Included for API compatibility with supervised methods.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : QRCP\n",
    "            Fitted object with Q, R, P, and selected_features_ populated.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # STEP 1: Compute QR decomposition with column pivoting\n",
    "        t_qr_start = time.time()\n",
    "        Q, R, piv = qr(X, pivoting=True, mode='economic')\n",
    "        self.runtime_['qr_decomposition'] = time.time() - t_qr_start\n",
    "\n",
    "        # Store decomposition results\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        self.P = np.array(piv)\n",
    "\n",
    "        # STEP 2: Determine selected features (top-k pivoted columns)\n",
    "        n_features = X.shape[1]\n",
    "        k = self.top_k if self.top_k is not None else n_features\n",
    "        self.selected_features_ = self.P[:k]\n",
    "\n",
    "        # Record total runtime\n",
    "        self.runtime_['total'] = time.time() - start_time\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform X by selecting the top-k pivoted columns.\n",
    "        \"\"\"\n",
    "        if self.selected_features_ is None:\n",
    "            raise ValueError(\"Must call fit() before transform()\")\n",
    "        return X[:, self.selected_features_]\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray, y=None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convenience method to fit and transform in one step.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "class LassoSelector:\n",
    "    \"\"\" \n",
    "    Lasso selector trains lasso cv to zero out the weights of the unimportant \n",
    "    features and then it selects the non zero features which are the important ones and \n",
    "    returns a dataset with only those features. \n",
    "    \"\"\"\n",
    "    # sets up selector\n",
    "    def __init__(self, k=20, cv=5, random_state=None):\n",
    "        # number of features to select\n",
    "        self.k = k\n",
    "        # number of folds for cross validation \n",
    "        self.cv = cv\n",
    "        self.random_state = random_state\n",
    "        self.model = LassoCV(cv=self.cv, n_jobs=-1, max_iter=5000, random_state=self.random_state)\n",
    "        self.selected_idx_ = None\n",
    "\n",
    "    #trains the model and gets weights\n",
    "    def fit(self, X, y):\n",
    "        # fit LassoCV and select top-k features\n",
    "        self.model.fit(X, y)\n",
    "        coefs = np.abs(self.model.coef_)\n",
    "\n",
    "        # incase k is all zero, select first k features\n",
    "        if np.all(coefs == 0):\n",
    "            self.selected_idx_ = np.arange(min(self.k, X.shape[1]))\n",
    "        else:\n",
    "            # select top-k features by magnitude\n",
    "            self.selected_idx_ = np.argsort(coefs)[-self.k:]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # returns new dataset with only selected features\n",
    "    def transform(self, X):\n",
    "        # reduce X to the selected features\n",
    "        if self.selected_idx_ is None:\n",
    "            raise RuntimeError(\"You must call fit() before transform().\")\n",
    "        return X[:, self.selected_idx_]\n",
    "\n",
    "    # combines fit and transform into one step\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "    # shows the features that were selected\n",
    "    def get_support(self, indices=False):\n",
    "        if indices:\n",
    "            return self.selected_idx_\n",
    "        mask = np.zeros(self.model.coef_.shape, dtype=bool)\n",
    "        mask[self.selected_idx_] = True\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluation Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Data Generators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Quick Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T21:22:59.741238Z",
     "iopub.status.busy": "2025-10-23T21:22:59.740932Z",
     "iopub.status.idle": "2025-10-23T21:22:59.946876Z",
     "shell.execute_reply": "2025-10-23T21:22:59.945480Z",
     "shell.execute_reply.started": "2025-10-23T21:22:59.741213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_simple_experiment():\n",
    "    \"\"\"Runs a basic test of the TAAQ class.\"\"\"\n",
    "\n",
    "    print(\"--- Running Simple TAAQ Experiment ---\")\n",
    "\n",
    "    # 1. Generate synthetic data\n",
    "    print(\"Generating synthetic data...\")\n",
    "    m, n = 500, 100 # Samples, Features\n",
    "    k_true = 15     # Number of informative features in the data\n",
    "    X, y = make_regression(n_samples=m, n_features=n, n_informative=k_true,\n",
    "                            noise=10, random_state=42)\n",
    "    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    # 2. Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Test shapes:  X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "    # 3. Instantiate and Fit TAAQ\n",
    "    k_select = 20 # Target number of features to select\n",
    "    print(f\"\\nInitializing TAAQ to select k={k_select} features...\")\n",
    "    taaq_selector = TAAQ(k=k_select, k1_fraction=0.5, delta=5, supervised=True)\n",
    "\n",
    "    print(\"Fitting TAAQ selector on training data...\")\n",
    "    start_fit = time.time()\n",
    "    taaq_selector.fit(X_train, y_train)\n",
    "    fit_time = time.time() - start_fit\n",
    "    print(f\"Fit completed in {fit_time:.4f} seconds.\")\n",
    "\n",
    "    # Print selected features and runtime breakdown\n",
    "    print(\"Selected feature indices:\", taaq_selector.selected_features_)\n",
    "    print(\"Runtime breakdown (seconds):\")\n",
    "    for phase, t in taaq_selector.runtime_.items():\n",
    "        print(f\"  - {phase}: {t:.4f}\")\n",
    "\n",
    "    # 4. Transform the data\n",
    "    print(\"\\nTransforming train and test data using selected features...\")\n",
    "    X_train_taaq = taaq_selector.transform(X_train)\n",
    "    X_test_taaq = taaq_selector.transform(X_test)\n",
    "    print(f\"Transformed train shape: {X_train_taaq.shape}\")\n",
    "    print(f\"Transformed test shape:  {X_test_taaq.shape}\")\n",
    "\n",
    "    # 5. Train and evaluate a simple Ridge model\n",
    "    print(\"\\nTraining Ridge model on TAAQ-selected features...\")\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train_taaq, y_train)\n",
    "\n",
    "    print(\"Evaluating model on transformed test data...\")\n",
    "    y_pred = model.predict(X_test_taaq)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"\\nR² score on test set using TAAQ features: {r2:.4f}\")\n",
    "\n",
    "    # --- Optional: Compare with using all features ---\n",
    "    print(\"\\nTraining Ridge model on ALL features for comparison...\")\n",
    "    model_all = Ridge(alpha=1.0)\n",
    "    # Need to scale data if comparing fairly, especially for Ridge\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model_all.fit(X_train_scaled, y_train)\n",
    "    y_pred_all = model_all.predict(X_test_scaled)\n",
    "    r2_all = r2_score(y_test, y_pred_all)\n",
    "    print(f\"R² score on test set using ALL features: {r2_all:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Experiment Finished ---\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Run the experiment\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    run_simple_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Main Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
